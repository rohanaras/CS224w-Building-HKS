{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SF Heat Kernel Signature + Persistence Based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "from heat_kernel_func import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load city data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luRE = pd.read_csv('../sanfrancisco/luRetailEnt_utm.csv')\n",
    "print('Number of buildings:', len(luRE.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Basemap(\n",
    "        projection='merc',\n",
    "        llcrnrlon=-122.54,\n",
    "        llcrnrlat=37.7,\n",
    "        urcrnrlon=-122.34,\n",
    "        urcrnrlat=37.83,\n",
    "        lat_ts=0,\n",
    "        resolution='h',\n",
    "        suppress_ticks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findking $k$\n",
    "\n",
    "One issue here is that we're forcing one connected component, but there are certain parts of the city that are really quite geographically isolated from the rest. For example, it is very difficult to connect the group of buildings in the far northeast of the city to the rest of the buildings.\n",
    "\n",
    "Is there a way to choose the number of components that allows for these small isolated groups? Or do we not care; we'd rather make a fully connected graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_neighbors(points, weighted=True, k=3):\n",
    "    '''\n",
    "    weights are distance\n",
    "    returns a weighted edgelist\n",
    "    '''\n",
    "    num_points = points.shape[0]\n",
    "    \n",
    "    # expanded out form of distance formula\n",
    "    squared = (points ** 2).sum(axis=1, keepdims=True)\n",
    "    mixed = points @ points.T\n",
    "    dists = np.sqrt(squared + squared.T - (2 * mixed))\n",
    "\n",
    "    closest_idx = np.argsort(dists, axis=1)[:, 1:(k + 1)]\n",
    "    \n",
    "    edge_list = []\n",
    "    for i in range(num_points): # this can probably be parallelized\n",
    "        cdists = dists[i, closest_idx[i]]\n",
    "        if weighted:\n",
    "            edge_list.extend([(i, j, cdists[k]) \n",
    "                              for k, j in enumerate(closest_idx[i])])\n",
    "        else:\n",
    "            edge_list.extend([(i, j, 1) \n",
    "                              for k, j in enumerate(closest_idx[i])])\n",
    "\n",
    "    \n",
    "    del dists\n",
    "    del closest_idx\n",
    "    \n",
    "    return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "weighted_edge_list = k_neighbors(luRE[['utm10_x', 'utm10_y']].values, k=k)\n",
    "weighted_G = nx.Graph()\n",
    "weighted_G.add_weighted_edges_from(weighted_edge_list)\n",
    "components = nx.number_connected_components(weighted_G)\n",
    "while components > 1:\n",
    "    k += 1\n",
    "    weighted_edge_list = k_neighbors(luRE[['utm10_x', 'utm10_y']].values, k=k)\n",
    "    weighted_G = nx.Graph()\n",
    "    weighted_G.add_weighted_edges_from(weighted_edge_list)\n",
    "    components = nx.number_connected_components(weighted_G)\n",
    "print('k =', k)\n",
    "print('Number of connected components:', components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utm\n",
    "plt.figure(figsize=(20, 20), dpi=150)\n",
    "m.readshapefile('../sanfrancisco/shapefiles/geo_export_0009bec5-e498-43e4-a53e-e167d066c874', \n",
    "               'geo_export_0009bec5-e498-43e4-a53e-e167d066c874')\n",
    "luRE['latlon'] = [utm.to_latlon(pair[1], pair[2], 10, 'U') for pair in luRE[['utm10_x', 'utm10_y']].itertuples()]\n",
    "positions = {i: m(pair[1][1], pair[1][0]) \n",
    "            for i, pair in enumerate(luRE[['latlon']].itertuples())}\n",
    "nx.draw_networkx(weighted_G, positions, node_size=5, with_labels=False, alpha=.7, width=.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $t$ threshold\n",
    "\n",
    "\n",
    "At a certain $t$, the number of clusters when $\\tau=0$ increases sharply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = list(np.logspace(-3, 4, num=21)) # extensive search\n",
    "#ts = list(np.linspace(0.36, 0.38, num=21)) # exact threshold\n",
    "weighted_hks_dict = {}\n",
    "weighted_hks_dict = hks_s(weighted_G, ts, list(weighted_G.nodes()), verbose=True, k=2000,\n",
    "                          eigen_save_loc='eigen_saves/weighted_sf_knn_norm')\n",
    "def get_num_clusters(G, hks_dict, t):\n",
    "    _,PD_points = persistence_diagram(G, f=hks_dict[t])\n",
    "    C, _ = persistence_diagram(G, f=hks_dict[t], tau=0)\n",
    "    return len(PD_points.keys()), len(set(get_root(node, C) for node in C))\n",
    "num_clusters = []\n",
    "c_set_size = []\n",
    "for t in ts:\n",
    "    points, cpoints = get_num_clusters(weighted_G, weighted_hks_dict, t=t)\n",
    "    num_clusters.append(points)\n",
    "    c_set_size.append(cpoints)\n",
    "plt.figure()\n",
    "#plt.plot(ts, num_clusters)\n",
    "plt.plot(ts, c_set_size, 'o', linestyle='-')\n",
    "plt.xlabel('Time (t) value')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Total number of clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_set_size)\n",
    "print (ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring how different values of $t$ affect clustering with $\\tau=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def explore_components(G, hks_dict, t, tau):\n",
    "    _, PD_points = persistence_diagram(G, f=hks_dict[t])\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Persistence Diagram (t = ' + str(t) + ', tau = ' + str(tau) + ')')\n",
    "    plot_PD(PD_points, tau=tau)\n",
    "    plt.show()\n",
    "    C, _ = persistence_diagram(G, tau=tau, f=hks_dict[t])\n",
    "    plt.figure(figsize=(20, 20), dpi=150)\n",
    "    m.readshapefile('../sanfrancisco/shapefiles/geo_export_0009bec5-e498-43e4-a53e-e167d066c874', \n",
    "               'geo_export_0009bec5-e498-43e4-a53e-e167d066c874')\n",
    "    plot_segments(G, C, positions, node_size=10, width=.15)\n",
    "    return len(PD_points.keys())\n",
    "ts = [10e-300, 10e-10, 0.37]\n",
    "weighted_hks_dict = hks_s(weighted_G, ts, list(weighted_G.nodes()), verbose=True, k=2000,\n",
    "                          eigen_save_loc='eigen_saves/weighted_sf_knn_norm')\n",
    "for t in ts:\n",
    "    explore_components(weighted_G, weighted_hks_dict, t, tau=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring how clusters persist as we increase $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "taus = [0.01, 0.1, 0.5]\n",
    "for tau in taus:\n",
    "    explore_components(weighted_G, weighted_hks_dict, t=1e-9, tau=tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_sizes(C):\n",
    "    from collections import defaultdict\n",
    "    counter = defaultdict(int)\n",
    "    for node in C:\n",
    "        counter[get_root(node, C)] += 1\n",
    "    return counter\n",
    "\n",
    "bwah = []\n",
    "for t in ts:\n",
    "    C, _ = persistence_diagram(weighted_G, tau=0, f=weighted_hks_dict[t])\n",
    "    gcs = get_cluster_sizes(C)\n",
    "    bwah.append(sum(gcs.values()) / len(gcs.values()))\n",
    "plt.plot(ts, bwah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainEnv",
   "language": "python",
   "name": "mainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
